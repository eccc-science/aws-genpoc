
# rule samtools_view:
#     input:
#        s3.remote("agc-852085885259-ca-central-1/mydata/{sample}.sam")
#     output:
#         bam="bam/{sample}.bam",
#         idx="bam/{sample}.bai",
#     log:
#         "{sample}.log",
#     params:
#         extra="",  # optional params string
#         region="",  # optional region string
#     threads: 2
#     wrapper:
#         "v3.3.3/bio/samtools/view"


# rule sam_to_bam:
#     input:
#         sam=expand("outputs/sam/{sample}.sam", sample=CONDITIONS)
#     output:
#         bam=expand("outputs/sam/{sample}.bam", sample=CONDITIONS)
#     conda:
#        "envs/environment.yaml",
#     threads: 12
#     resources:
#        mem_mb=64000
#     shell:
#         "samtools view -Sb {input.sam} | samtools sort sort -@ 12 -m 5M -o {output.bam}"

# rule index_bam:
#     input:
#         bam=rules.sam_to_bam.output.bam
#     output:
#         bai=expand("outputs/sam/{sample}.bai", sample=CONDITIONS)
#     threads: 12
#     resources:
#        mem_mb=64000
#     conda:
#        "envs/environment.yaml",
#     shell:
#         "samtools index {input.bam}"


#  Assuming we want the trimmed file.  Could use other tools to convert it to a sam file
#rule reformat:
#    input:
#        r1="trimmed/{sample}_{req}.fastq"
#    output:
#        #r1=pipe("sam/{sample}_1.sam")
#        r1="sam/{sample}_1.sam"
#    conda:
#       workflow_env
#    shell:
#        "reformat.sh in={input.r1} out={output.r1} overwrite=true"

#Assembles fastq files using default settings
# rule spades:
#    input:
#        r1 = expand("{trimmed}/output_forward_paired_{sample}.fq.gz",trimmed=TRIM_FOLDER, sample=CONDITIONS),
#        r2 = expand("{trimmed}/output_reverse_paired_{sample}.fq.gz",trimmed=TRIM_FOLDER, sample=CONDITIONS)
#    output:
#        "spades_assemblies/{sample}/contigs.fasta"
#    log: "logs/spades.{sample}.log"
#    conda: "envs/spades.yaml"
#    threads: 16
#    shell:
#       "spades.py -t {threads} -1 {input.r1} -2 {input.r2} -o spades_assemblies/{wildcards.sample} &>{log}"


### Alternative method to doing the download and makedb in the workflow.  This is not recommended for larger databases as it will download the large file each time the workflow is run.  It is included for reference only. #
## Since NR is 300 GB it was pre-downloaded and stored as a diamond DB at s3://genpocdata-992085379228-ca-central-1/Reference-Databases/nr.dmnd to save time ###
## could look into using AWS Open data for this https://aws.amazon.com/marketplace/pp/prodview-uutdnhlrfc4ym or ElasticBlast ### 
# rule download_reference:
#    input:
#       # forward reads
#       forward_paired = expand("{trimmed}/output_forward_paired_{sample}.fq.gz",trimmed=TRIM_FOLDER, sample=CONDITIONS),
#       forward_unpaired = expand("{trimmed}/output_forward_unpaired_{sample}.fq.gz",trimmed=TRIM_FOLDER, sample=CONDITIONS),
#       # Backward reads
#       backward_paired = expand("{trimmed}/output_reverse_paired_{sample}.fq.gz",trimmed=TRIM_FOLDER, sample=CONDITIONS),
#       backward_unpaired = expand("{trimmed}/output_reverse_unpaired_{sample}.fq.gz",trimmed=TRIM_FOLDER, sample=CONDITIONS)
#    output:
#       "outputs/reference/{INPUT_FASTA}"
#    shell:
#       """
#       wget {INPUT_FASTA_URL} -P "reference"
#       gunzip outputs/reference/{INPUT_FASTA}.{INPUT_FASTA_ZIP_EXT}
#       rm -f outputs/reference/{INPUT_FASTA}.{INPUT_FASTA_ZIP_EXT}
#       """

# rule create_diamond_db:
#     input:
#         fasta=expand("reference/{input_fasta}", input_fasta=INPUT_FASTA)
#     output:
#         db={OUTPUT_DB}
#     conda:
#         "envs/environment.yaml"
#     shell:
#         """
#         diamond makedb --in {input.fasta} --db outputs/reference/{output.db}
#         rm -f {input.fasta}
#         """